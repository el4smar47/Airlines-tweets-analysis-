{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining\n",
    "\n",
    "\n",
    "## Tweets preprocessing for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Twitter is a mix of social network and microblogging. In this network, people post information and communicate among themselves through messages, called tweets, that can contain up to 280 characters. In this project, *I will implement a prototype that can detect if an airline company is positively or negatively mentioned in a tweet*, and apply it for flight companies.\n",
    "\n",
    "Preprocessing is a crucial task in data mining. It transforms raw data into a format suitable for the application of machine learning methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Sentiment Analysis Model\n",
    "\n",
    "In the literature, the task of extracting the sentiment of a text is called *sentiment analysis*. I will implement a bag-of-words (BoW) model for this task.\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "For this project, I used `numpy`, `sklearn` and `scipy` libraries , but also the `nltk` library, which is used to do Natural Language Processing (NLP).\n",
    "\n",
    "\n",
    "The code below is used to install the packages needed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohamed Ali\n",
      "[nltk_data]     Elakhras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohamed Ali\n",
      "[nltk_data]     Elakhras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mohamed Ali\n",
      "[nltk_data]     Elakhras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Mohamed\n",
      "[nltk_data]     Ali Elakhras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohamed Ali\n",
      "[nltk_data]     Elakhras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# !pip install --user numpy\n",
    "# !pip install --user sklearn\n",
    "# !pip install --user scipy\n",
    "# !pip install --user nltk\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"universal_tagset\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "I used a database that comes from *Crowdflower's Data for Everyone library*: https://www.figure-eight.com/data-for-everyone/\n",
    "\n",
    "To quote the original source :\n",
    "\n",
    "    A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "\n",
    "The companies included in this database are Virgin America, United Airline, Southwest Airlines, jetBlue, USAirways, and American Airlines.\n",
    "\n",
    "In the project zip file, you will find the *airline_tweets_database.csv* file, which is the tweets database that I will manipulate.\n",
    "\n",
    "Each line of this file contains a tweet, with several information: the tweet identifier, the user, the content, the number of retweet.... But also the label, which is the sentiment of the tweet.\n",
    "\n",
    "3 different labels are possible in this dataset: *negative*, *neutral* and *positive*, which I will represent respectively by 0, 1 and 2.\n",
    "\n",
    "For this project, only the text and the label will be kept here. The database is then divided into 3 sets (training/validation/test).I will use the training and validation set for this part, and the test set for the next part.\n",
    "\n",
    "The code below allows me to load the datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set :  10204\n",
      "Length of validation set :  2240\n",
      "Length of test set :  2196\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(path):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"latin-1\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('airline_sentiment')\n",
    "        ind_text = header.index('text')\n",
    "        \n",
    "        for row in reader:\n",
    "            x.append(row[ind_text])\n",
    "            \n",
    "            label = row[ind_label]\n",
    "            \n",
    "            if label == \"negative\":\n",
    "                y.append(0)\n",
    "            elif label == \"neutral\":\n",
    "                y.append(1)\n",
    "            elif label == \"positive\":\n",
    "                y.append(2)\n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Path of the dataset\n",
    "path = \"data/airline_tweets_database.csv\"\n",
    "\n",
    "X, y = load_dataset(path)\n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "In NLP, *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, I will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "\n",
    "### 3.1. Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). For instance, the sentence *\"It's the man's notebook.\"* can be split into the following list of tokens: ['It', \"'s\", 'the', 'man', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "##### Implemention of the SpaceTokenizer and NLTKTokenizer tokenizers.\n",
    " \n",
    "- **SpaceTokenizer** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "- **NLTKTokenizer** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "class SpaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "\n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        splitTweet = re.split(' |\\n|\\t', text)         \n",
    "            \n",
    "        # Have to return a list of tokens\n",
    "        return splitTweet\n",
    "\n",
    "\n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        tweetTokenizer = TweetTokenizer()\n",
    "        splitTweet = tweetTokenizer.tokenize(text)\n",
    "\n",
    "        # Have to return a list of tokens\n",
    "        return splitTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test both tokenizers. What differences can I see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@usairways', 'tells', 'me', 'that', 'yed', 'is', 'my', 'best', 'friend!!!!', 'i', 'love', 'this', 'guy', 'so', 'much', ':d', '...']\n",
      "\n",
      "\n",
      "['@usairways', 'tells', 'me', 'that', 'yed', 'is', 'my', 'best', 'friend', '!', '!', '!', 'i', 'love', 'this', 'guy', 'so', 'much', ':d', '...']\n",
      "\n",
      "\n",
      "The difference is that all the ponctuations are tokenized on their own in the NLTK default function Also, all the special characters like '%', $, ... (except for the @ and the #) are considered as tokens.\n"
     ]
    }
   ],
   "source": [
    "spaceTokenizer = SpaceTokenizer()\n",
    "print(spaceTokenizer.tokenize(\"@USAirways tells me that yed is my best friend!!!! I love this guy so much :D ...\"))\n",
    "print('\\n')\n",
    "nlktTokenizer = NLTKTokenizer()\n",
    "print(nlktTokenizer.tokenize(\"@USAirways tells me that yed is my best friend!!!! I love this guy so much :D ...\"))\n",
    "print('\\n')\n",
    "print(\"The difference is that all the ponctuations are tokenized on their own in the NLTK default function \\\n",
    "Also, all the special characters like '%', $, ... (except for the @ and the #) are considered as tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Stemming\n",
    "\n",
    "In the tweets *\"I should have bought a new shoes today\"* and *\"I spent too much money buying games\"*, the words *\"buy\"* and *\"bought\"* represent basically the same concept. Considering both words as different can unnecessarily increase the dimensionality of the problem and can negatively impact the performance of simple models. Therefore, a unique form (e.g., the root buy) can represent both words. The process to convert words with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*.\n",
    "\n",
    "##### I retrieve the stems of the tokens using the attribute *stemmer* from the class *Stemmer*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "class Stemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "    def stem(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens: a list of strings\n",
    "        \"\"\"\n",
    "        stems = map(self.stemmer.stem, tokens)\n",
    "        # Have to return a list of stems\n",
    "        return list(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tell',\n",
       " 'me',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'about',\n",
       " 'my',\n",
       " 'delay',\n",
       " 'flight',\n",
       " '.',\n",
       " 'aa',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'us',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "stemmer.stem(['tells', 'me', 'to', 'talk', 'to', 'about', 'my', 'delayed', 'flights', '.', 'aa', 'tells', 'me', 'to', 'talk', 'to', 'us', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Twitter preprocessing\n",
    "\n",
    "Sometimes only applying the default NLP preprocessing steps is not enough. Data for certain domains can have peculiar characteristics which requires specific preprocessing steps to remove the noise and create a more suitable format for the models. \n",
    "\n",
    "In NLP, methods store a set of words, called dictionary, and all the words out of the dictionary are considered as unknown. In this assignment, the feature space dimensionality of a model is directly related to the number of words in the dictionary. Since high-dimensional spaces can suffer from the curse of dimensionality, our goal is to create preprocessing steps that decrease vocabulary size.  \n",
    "\n",
    "#### I briefly explain and implement at least two preprocessing steps that reduce the dictionary size (number of unique words). \n",
    "\n",
    "I decided that these preprocessing steps must be related to the specific characteristic of the Twitter data. Therefore, for instance, I should not use the stop words removal as a preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "class TwitterPreprocessing(object):\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: original tweet\n",
    "        \"\"\"\n",
    "        punctuations = ['.', ',', '?', '!', \"'\", '\"', ';', ':', '...', '-', '_', '(', ')', '[', ']']\n",
    "        new_tweet = []\n",
    "        for word in tweet:\n",
    "            if word.find('http') == -1 and word not in punctuations and not re.search(r'\\d', word):\n",
    "                new_tweet.append(word)\n",
    "       \n",
    "        return new_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':d', 'dsad']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterPreprocessing = TwitterPreprocessing()\n",
    "twitterPreprocessing.preprocess([':d', 'dsad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.  Pipeline\n",
    "\n",
    "The pipeline is sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. I implement the class *PreprocessingPipeline* that applies the tokenizer, twitter preprocessing and stemer to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "\n",
    "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "        self.twitterPreprocesser = TwitterPreprocessing(\n",
    "        ) if twitterPreprocessing else None\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        twitterPreprocessing: boolean value. Apply the\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "\n",
    "        if self.twitterPreprocesser:\n",
    "            tokens = self.twitterPreprocesser.preprocess(tokens)\n",
    "        \n",
    "        if self.stemmer:\n",
    "            tokens = self.stemmer.stem(tokens)   \n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@usairway', 'tell', 'me', 'to', 'talk', 'to', '@americanair', 'about', 'my', 'delay', 'flight', 'aa', 'tell', 'me', 'to', 'talk', 'to', 'us', '#ihatemerg']]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "print(list(map(pipeline.preprocess, train_X[:1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. N-grams\n",
    "\n",
    "An n-gram is a contiguous sequence of *n* tokens from a text. Thus, for instance,the sequence *\"bye as\"* and *\"walked through\"* are example of 2-grams from the sentence *\"He said bye as he walked through the door .\"*. 1-gram, 2-gram and 3-gram are, respectively, called unigram, bigram and trigram. I list all the possible unigram, bigram and trigram from the *\"He said bye as he walked through the door .\"*:\n",
    "\n",
    "- Unigram: [\"He\", \"said\", \"bye\", \"as\", \"he\", \"walked\", \"through\", \"the\", \"door\", \".\"]\n",
    "- Bigram: [\"He said\", \"said bye\", \"bye as\", \"as he\", \"he walked\", \"walked through\", \"through the\", \"the door\", \"door .\"] \n",
    "- Trigram: [\"He said bye\", \"said bye as\", \"bye as he\", \"as he walked\", \"he walked through\", \"walked through the\", \"through the door\", \"the door .\"] \n",
    "\n",
    "\n",
    "##### Implementation of `bigram` and `trigram`. \n",
    "\n",
    "For this exercise, I decided not to use any external python library (e.g., scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # Pair the tokens two by two\n",
    "    bigrams = zip(*[tokens[i:] for i in range(2)])\n",
    "    # Join the pair of words with a space\n",
    "    bigrams = [\" \".join(bigram_1) for bigram_1 in bigrams]\n",
    "    \n",
    "    # This function returns the list of bigrams\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def trigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    #Pair the tokens three by three\n",
    "    trigrams = zip(*[tokens[i:] for i in range(3)])\n",
    "    # Join the pair of words with a space\n",
    "    trigrams = [\" \".join(trigram_1) for trigram_1 in trigrams]\n",
    "    \n",
    "    # This function returns the list of trigrams\n",
    "    return trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my name', 'name is', 'is jad']\n"
     ]
    }
   ],
   "source": [
    "test = ['@americanair', 'i', 'do', 'not', 'want', 'to', 'stay', 'overnight', 'at', 'this', 'airport', '!', 'i', 'have', 'been', 'here', 'since', '11', 'am', 'this', 'morning', '!']\n",
    "test2 = ['my', 'name', 'is', 'jad']\n",
    "print(bigram(test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bag-of-words\n",
    "\n",
    "Logistic regression, SVM and other well-known models only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two tweets: ”Board games are much better than video games” and ”Pandemic is an awesome game!”. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|  <i></i>   | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "##### Implementation of the bag-of-words model that weights the vector with the absolute word frequency. \n",
    "\n",
    "For this step, I decided not to use any external python library (e.g., scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "class CountBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "\n",
    "    def computeBoW(self, X):\n",
    "        \"\"\"\n",
    "        BoW calculation, based on a dictionary of words and a list of tweets.\n",
    "        I assume that the dictionary has already been collected on the training set.\n",
    "        \n",
    "        Entry: X, a list of vectors containing tweets\n",
    "        \n",
    "        Return: a csr_matrix\n",
    "        \"\"\"\n",
    "        if self.words is None:\n",
    "            \n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        \n",
    "        csr_matrix = np.zeros((len(X), len(self.words)))\n",
    "        \n",
    "        bigram_tokens = [] \n",
    "        trigram_tokens = []\n",
    "        for i, tweet in enumerate(X):\n",
    "            preprocessed_tweet = self.pipeline.preprocess(tweet)\n",
    "            if self.bigram:\n",
    "                bigram_tokens = bigram(preprocessed_tweet)\n",
    "            if self.trigram:\n",
    "                trigram_tokens = trigram(preprocessed_tweet)\n",
    "            preprocessed_tweet = preprocessed_tweet + bigram_tokens + trigram_tokens\n",
    "            for word in preprocessed_tweet:\n",
    "                try:\n",
    "                    j = self.words.index(word)\n",
    "                    csr_matrix[i][j] += 1    \n",
    "                except ValueError:\n",
    "                    pass\n",
    "                                       \n",
    "        return csr_matrix\n",
    "        \n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses data using the pipeline, adds bigram and trigram \n",
    "        if necessary, and transforms texts into whole vectors.\n",
    "        \n",
    "        Entry: X, a list of vectors containing tweets\n",
    "        \n",
    "        Return: a csr_matrix\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        self.words = []\n",
    "        bigram_list = []\n",
    "        trigram_list = []\n",
    "        for tweet in X:\n",
    "            preprocessed_tweet = self.pipeline.preprocess(tweet)\n",
    "            self.words.extend(preprocessed_tweet)\n",
    "            if self.bigram:\n",
    "                bigram_list.extend(bigram(preprocessed_tweet))\n",
    "            if self.trigram:\n",
    "                trigram_list.extend(trigram(preprocessed_tweet))\n",
    "        self.words = list(set(self.words)) # I remove the duplicates\n",
    "\n",
    "        # I add the bigrams and trigrams to the vocabulary\n",
    "        if self.bigram:\n",
    "            bigram_list = list(set(bigram_list)) # I remove the duplicates\n",
    "            self.words += bigram_list\n",
    "        if self.trigram:\n",
    "            trigram_list = list(set(trigram_list)) # I remove the duplicates\n",
    "            self.words += trigram_list \n",
    "        \n",
    "        print(\"Size of the dictionary: \" + str(len(self.words)))\n",
    "        \n",
    "        return self.computeBoW(X)\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses data using the pipeline, adds bigram and trigram \n",
    "        if necessary, and transforms texts into whole vectors.\n",
    "        \n",
    "        Difference with fit_transform: we assume that we already have the dictionary here\n",
    "\n",
    "        Entry: X, a list of vectors containing tweets\n",
    "        \n",
    "        Return: a csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        \n",
    "        return self.computeBoW(X)\n",
    "    \n",
    "    def get_dic(self):\n",
    "        return self.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TF-IDF\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of tweets. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\n",
    "\\begin{equation}\n",
    "\tidf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of documents in the dataset, $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "\n",
    "where $tf_{ij}$ is the term frequency of word $i$ in the document $j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Implementation of a bag-of-words model that weights the vector using TF-IDF.\n",
    "\n",
    "For this step, I choose not to use any external python library (e.g., scikit-learn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "\n",
    "class TFIDFBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        idf: list of idfs for each document\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "        self.idf = None\n",
    "        self.tf_df = None\n",
    "        \n",
    "    def add_word_tf_df(self, word):\n",
    "        try:\n",
    "            self.tf_df[word] = self.tf_df[word] + 1\n",
    "        except:\n",
    "            self.tf_df[word] = 1\n",
    "            \n",
    "        return word\n",
    "\n",
    "    \n",
    "    def calculate_df(self, X, list_bigram, list_trigram):\n",
    "        for tweet in X:\n",
    "            tweet = self.pipeline.preprocess(tweet)\n",
    "            tweet = list(set(tweet)) # On enleve les doublons\n",
    "            df = list(map(self.add_word_tf_df, tweet))\n",
    "            \n",
    "        if len(list_bigram) > 0:\n",
    "            df += list(map(self.add_word_tf_df, list_bigram))\n",
    "        if len(list_trigram) > 0:\n",
    "            df += list(map(self.add_word_tf_df, list_trigram))\n",
    "\n",
    "        return self.tf_df\n",
    "    \n",
    "    \n",
    "    def calculate_tf(self, tweet, list_bigram, list_trigram):\n",
    "        tweet = self.pipeline.preprocess(tweet)\n",
    "        tf = list(map(self.add_word_tf_df, tweet))\n",
    "        \n",
    "        if len(list_bigram) > 0:\n",
    "            tf += list(map(self.add_word_tf_df, list_bigram))\n",
    "        if len(list_trigram) > 0:\n",
    "            tf += list(map(self.add_word_tf_df, list_trigram))\n",
    "                \n",
    "        return self.tf_df\n",
    "    \n",
    "    \n",
    "    def computeTFIDF(self, X):\n",
    "        \"\"\"\n",
    "        Calculates the TF-IDF, using a dictionary of words and a \n",
    "        list of tweets.\n",
    "        It is assumed that the dictionary has already been collected as well as \n",
    "        calculated the vector containing the idf for each document.\n",
    "        \n",
    "        Entry: X, a list of vectors containing tweets\n",
    "        \n",
    "        Return: a csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        \n",
    "        csr_matrix = np.zeros((len(X), len(self.words)))\n",
    "        for i, tweet in enumerate(X):\n",
    "            self.tf_df = {}\n",
    "            bigram_tokens = []\n",
    "            trigram_tokens = []\n",
    "            preprocessed_tweet = self.pipeline.preprocess(tweet)\n",
    "            if self.bigram:\n",
    "                bigram_tokens = bigram(preprocessed_tweet)\n",
    "            if self.trigram:\n",
    "                trigram_tokens = trigram(preprocessed_tweet)\n",
    "            preprocessed_tweet = preprocessed_tweet + bigram_tokens + trigram_tokens\n",
    "    \n",
    "            tf = self.calculate_tf(tweet, bigram_tokens, trigram_tokens)\n",
    "\n",
    "            for word in preprocessed_tweet:\n",
    "                try:\n",
    "                    index = self.words.index(word)\n",
    "                    csr_matrix[i][index] = tf[word] * self.idf[word]\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        return csr_matrix      \n",
    "        \n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses data using the pipeline, adds bigram and trigram \n",
    "        if necessary, and transforms the texts into floating vectors with the TF-IDF weighting.\n",
    "        \n",
    "        Entry: X, a list of vectors containing tweets\n",
    "        \n",
    "        Return: a csr_matrix\n",
    "        \"\"\"\n",
    "        self.tf_df = {}\n",
    "        self.words = []\n",
    "        bigram_list = []\n",
    "        trigram_list = []\n",
    "        for tweet in X:\n",
    "            preprocessed_tweet = self.pipeline.preprocess(tweet)\n",
    "            self.words.extend(preprocessed_tweet)\n",
    "            if self.bigram:\n",
    "                bigram_list.extend(bigram(preprocessed_tweet))\n",
    "            if self.trigram:\n",
    "                trigram_list.extend(trigram(preprocessed_tweet))\n",
    "        self.words = list(set(self.words)) # On retire les doublons\n",
    "\n",
    "        # I add the bigrams and trigrams to the vocabulary\n",
    "        if self.bigram:\n",
    "            bigram_list = list(set(bigram_list)) # Removing the duplicates\n",
    "            self.words += bigram_list\n",
    "        if self.trigram:\n",
    "            trigram_list = list(set(trigram_list)) # Removing the duplicates\n",
    "            self.words += trigram_list\n",
    "        \n",
    "        print(\"Size of the dictionary: \" + str(len(self.words)))\n",
    "        \n",
    "        self.idf = {}\n",
    "        df_values = self.calculate_df(X, bigram_list, trigram_list)\n",
    "        for word in self.words:\n",
    "            self.idf[word] = math.log(len(X)/df_values[word])\n",
    "            \n",
    "        return self.computeTFIDF(X)\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses data using the pipeline, adds bigram and trigram \n",
    "        if necessary, and transforms the texts into floating vectors with the TF-IDF weighting.\n",
    "        Difference with fit_transform: we assume that we already have the dictionary and calculation of the idf here.\n",
    "            \n",
    "        Entry: X, a list of vectors containing tweets\n",
    "        \n",
    "        Return: a csr_matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        return self.computeTFIDF(X)\n",
    "    \n",
    "    def get_dic(self):\n",
    "        return self.words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.69314718, 0.        , 0.        , 0.69314718, 0.69314718,\n",
       "        0.69314718, 0.69314718, 0.        , 0.69314718, 0.        ,\n",
       "        0.        , 0.69314718, 0.        , 0.        , 0.        ,\n",
       "        0.69314718, 0.69314718, 0.        , 0.69314718, 0.69314718,\n",
       "        0.69314718, 0.69314718, 0.69314718, 0.        , 0.        ,\n",
       "        0.69314718, 0.69314718, 0.69314718, 0.69314718, 0.        ,\n",
       "        0.69314718],\n",
       "       [0.        , 0.69314718, 0.69314718, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.69314718,\n",
       "        0.69314718, 0.        , 0.69314718, 0.69314718, 0.69314718,\n",
       "        0.        , 0.        , 0.69314718, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.69314718, 0.69314718,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.69314718,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [\"Board games are much better than video games\", \"Pandemic is an awesome game!\"]\n",
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "test_TFIDF = TFIDFBoW(pipeline, bigram=True, trigram=True)\n",
    "test_TFIDF.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classifier using BoW\n",
    "\n",
    "I am going to use logistic regression as a classifier. Read the following page to know more about this classifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "\n",
    "The method `train_evaluate` trains and evaluates the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "\n",
    "    classifier.fit(training_rep, training_Y)\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(\n",
    "        validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Train and calculate the logistic regression accuracy in the *training and validation dataset* using each one of the following configurations: \n",
    "\n",
    "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
    "    2. CountBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + unigram\n",
    "    4. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
    "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "    7. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
    "Besides the accuracy, I also report the dictionary size for each one of configurations. Finally, I describe the results found and answer the following questions:\n",
    "- Which preprocessing has helped the model? Why?\n",
    "- TF-IDF has achieved a better performance than CountBoW? \n",
    "- Has the bigram and trigram improved the performance?\n",
    "\n",
    "I then indicate which configuration I choose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 20564\n",
      "Time: 79.57876491546631\n",
      "Training Accuracy: 0.9572716581732654\n",
      "Validation Accuracy: 0.7986607142857143\n"
     ]
    }
   ],
   "source": [
    "# Configuration 1\n",
    "start_time = time.time()\n",
    "pipeline = PreprocessingPipeline(tokenization=False, twitterPreprocessing=False, stemming=False)\n",
    "countbow = CountBoW(pipeline)\n",
    "classifier, trainAcc, validationAcc = train_evaluate(train_X, train_Y, valid_X, valid_Y, countbow)\n",
    "end_time = time.time()\n",
    "print(\"Time: \" + str(end_time - start_time))\n",
    "print(\"Training Accuracy: \" + str(trainAcc))\n",
    "print(\"Validation Accuracy: \" +  str(validationAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 13170\n",
      "Time: 49.734052419662476\n",
      "Training Accuracy: 0.9351234809878479\n",
      "Validation Accuracy: 0.8035714285714286\n"
     ]
    }
   ],
   "source": [
    "# Configuration 2\n",
    "\n",
    "start_time = time.time()\n",
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=False, stemming=False)\n",
    "countbow = CountBoW(pipeline)\n",
    "classifier, trainAcc, validationAcc = train_evaluate(train_X, train_Y, valid_X, valid_Y, countbow)\n",
    "end_time = time.time()\n",
    "print(\"Time: \" + str(end_time - start_time))\n",
    "print(\"Training Accuracy: \" + str(trainAcc))\n",
    "print(\"Validation Accuracy: \" +  str(validationAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 13170\n",
      "Time: 37.59406852722168\n",
      "Training Accuracy: 0.9932379459035672\n",
      "Validation Accuracy: 0.7816964285714286\n"
     ]
    }
   ],
   "source": [
    "# Configuration 3\n",
    "start_time = time.time()\n",
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=False, stemming=False)\n",
    "bow = TFIDFBoW(pipeline)\n",
    "classifier, trainAcc, validationAcc = train_evaluate(train_X, train_Y, valid_X, valid_Y, bow)\n",
    "end_time = time.time()\n",
    "print(\"Time: \" + str(end_time - start_time))\n",
    "print(\"Training Accuracy: \" + str(trainAcc))\n",
    "print(\"Validation Accuracy: \" +  str(validationAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 10916\n",
      "Time: 33.90447521209717\n",
      "Training Accuracy: 0.9894159153273226\n",
      "Validation Accuracy: 0.7638392857142857\n"
     ]
    }
   ],
   "source": [
    "# Configuration 4\n",
    "start_time = time.time()\n",
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=False, stemming=True)\n",
    "bow = TFIDFBoW(pipeline)\n",
    "classifier, trainAcc, validationAcc = train_evaluate(train_X, train_Y, valid_X, valid_Y, bow)\n",
    "end_time = time.time()\n",
    "print(\"Time: \" + str(end_time - start_time))\n",
    "print(\"Training Accuracy: \" + str(trainAcc))\n",
    "print(\"Validation Accuracy: \" +  str(validationAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 8264\n",
      "Time: 23.158928155899048\n",
      "Training Accuracy: 0.9796158369266954\n",
      "Validation Accuracy: 0.7705357142857143\n"
     ]
    }
   ],
   "source": [
    "# Configuration 5\n",
    "start_time = time.time()\n",
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "bow = TFIDFBoW(pipeline)\n",
    "classifier, trainAcc, validationAcc = train_evaluate(train_X, train_Y, valid_X, valid_Y, bow)\n",
    "end_time = time.time()\n",
    "print(\"Time: \" + str(end_time - start_time))\n",
    "print(\"Training Accuracy: \" + str(trainAcc))\n",
    "print(\"Validation Accuracy: \" +  str(validationAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 76186\n",
      "Time: 417.9835731983185\n",
      "Training Accuracy: 0.9973539788318306\n",
      "Validation Accuracy: 0.7910714285714285\n"
     ]
    }
   ],
   "source": [
    "# Configuration 6\n",
    "start_time = time.time()\n",
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "bow = TFIDFBoW(pipeline, bigram=True)\n",
    "classifier, trainAcc, validationAcc = train_evaluate(train_X, train_Y, valid_X, valid_Y, bow)\n",
    "end_time = time.time()\n",
    "print(\"Time: \" + str(end_time - start_time))\n",
    "print(\"Training Accuracy: \" + str(trainAcc))\n",
    "print(\"Validation Accuracy: \" +  str(validationAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 197011\n",
      "Time: 1724.5353043079376\n",
      "Training Accuracy: 0.9973539788318306\n",
      "Validation Accuracy: 0.8017857142857143\n"
     ]
    }
   ],
   "source": [
    "# Configuration 7\n",
    "start_time = time.time()\n",
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "bow = TFIDFBoW(pipeline, bigram=True, trigram=True)\n",
    "classifier, trainAcc, validationAcc = train_evaluate(train_X, train_Y, valid_X, valid_Y, bow)\n",
    "end_time = time.time()\n",
    "print(\"Time: \" + str(end_time - start_time))\n",
    "print(\"Training Accuracy: \" + str(trainAcc))\n",
    "print(\"Validation Accuracy: \" +  str(validationAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/ Prototype \n",
    "\n",
    "Now that I have a trained classification model for feeling analysis, I can apply it to our set of tests and analyze the result.\n",
    "\n",
    "*For the prototype, I decided to use the best model found in the Section I/.*\n",
    "\n",
    "## 1. Sentiment analysis\n",
    "\n",
    "##### Implementation of the `detect_airline` function that detects the airline of a tweet. \n",
    "\n",
    "Then, I explain my approach, and the possible disadvantages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_airline(tweet):\n",
    "    \"\"\"\n",
    "    Detect and return the airline companies mentioned in the tweet\n",
    "    \n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    Return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    name_companies_bigram = { 'virgin america', 'united airlines', 'southwest airlines', 'american airlines' }\n",
    "    name_companies_unigram = { 'jetblue', 'usaairways'}\n",
    "    twitter_accounts = {'@americanair' : 'american airlines', '@virginamerica' : 'virgin america','@jetblue':'jetblue', \\\n",
    "                        '@southwestair': 'southwest airlines', '@unitedairlin': 'united airlines', \\\n",
    "                        '@united':'united airlines', '@jetbluecheep': 'jetblue', '@usair': 'usaairways', \\\n",
    "                        '@usairways': 'usaairways'}\n",
    "    abbrevs = {'aa':'american airlines','va':'virgin america','ua':'united airlines','sa':'southwest airlines', \\\n",
    "               'jb': 'jetblue'}\n",
    "\n",
    "    pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=False, stemming=False)\n",
    "    preprocess_tweet = pipeline.preprocess(tweet)\n",
    "    bigram_tweet = bigram(preprocess_tweet)\n",
    "    \n",
    "    list_companies = []\n",
    "    for word in preprocess_tweet:\n",
    "        if word in name_companies_unigram:\n",
    "            list_companies.append(word)\n",
    "        elif word in abbrevs:\n",
    "            list_companies.append(abbrevs.get(word, word))   \n",
    "        elif word in twitter_accounts:\n",
    "            list_companies.append(twitter_accounts.get(word, word))\n",
    "    \n",
    "    for gram in bigram_tweet:\n",
    "        if (gram in name_companies_bigram):\n",
    "            list_companies.append(gram)\n",
    "    \n",
    "    if len(list_companies) == 0:\n",
    "        list_companies.append(\"none\")\n",
    "    \n",
    "    return list_companies\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation of the method `extract_sentiment` that receives a tweet and extracts its sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(classifier, tweets):\n",
    "    \"\"\"\n",
    "    Extract the tweet sentiment\n",
    "    \n",
    "    classifier: classifier object\n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    Return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    return classifier.predict(tweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the *extract_tweet_content*, *detect_airline* and *extract_sentiment*, I implement a code that generates a bar chart that contains the number of positive, neutral and negatives tweets for each one of the companies. \n",
    "I briefly describe my bar chart (e.g, which was the company with most negative tweets) and how this chart can help airline companies.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My best configuration was Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 5407\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=False, stemming=False)\n",
    "countbow = CountBoW(pipeline)\n",
    "classifier = train_evaluate(test_X, test_Y, train_valid_X, train_valid_Y, countbow)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 5407\n"
     ]
    }
   ],
   "source": [
    "# Extract the data and append it to a dictionnary airlines_count\n",
    "sentiments = extract_sentiment(classifier,countbow.fit_transform(test_X))\n",
    "airlines_count = {'american airlines':{0:0, 1:0, 2:0}, 'virgin america':{0:0, 1:0, 2:0},'jetblue':{0:0, 1:0, 2:0}, \\\n",
    "                        'southwest airlines':{0:0, 1:0, 2:0}, 'united airlines':{0:0, 1:0, 2:0}, \\\n",
    "                         'usaairways':{0:0, 1:0, 2:0}, 'none':{0:0, 1:0, 2:0}}\n",
    "airlines_tweet = {'american airlines':[], 'virgin america':[],'jetblue':[], \\\n",
    "                        'southwest airlines':[], 'united airlines':[], \\\n",
    "                         'usaairways':[]}\n",
    "\n",
    "for i, tweet in enumerate(test_X):\n",
    "    list_companies = detect_airline(tweet)\n",
    "    for company in list_companies:\n",
    "        airlines_count[company][sentiments[i]] += 1\n",
    "        if (sentiments[i] == 0 or sentiments[i] == 2) and company != 'none':\n",
    "            airlines_tweet[company].append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAFZCAYAAACWmOQIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcXFWZ//HPlyQQQMISgixBEiAsIYQAAaLATwRkRxQTUFHDDm6oCIqOIDI6IjKAa0BByCBL2AR1mBEnsspmImEH2QIEEEKAsAYS8vz+OKdDpdNLdae7761b3/frVa+uureq7lPVt546de65z1FEYGZm1bVM0QGYmVnvcqI3M6s4J3ozs4pzojczqzgnejOzinOiNzOrOCf6gkl6XdL6fbCd5SX9UdJcSZf30jZ2lPRwbzx3B9s8WNItfbnNNmLYSdKsgrZ9sqTfFbHtvrK0+25nnzFJF0j6Qb7e5/twX2i6RJ8Tw72S3pT0L0m/krRyH237BkmH1y6LiPdFxON5/aIdrheMB94PDI6ICT3xhJJC0oYttyPi5ojYuCee25bU218ovbz/LY2l2ndrP2N13LeS+3BTJXpJ3wB+DBwPrAyMA4YB10kaUGBofWE94J8RsaCeO0vq38vxmNWrS/tuV0jq19PPWUoR0RQXYBDwOnBAq+XvA14AJubbFwA/qFm/EzCr5vYJwGPAa8ADwCdq1h0M3AKcDrwMPAHsmdf9EHgXmJfj+EVeHsCGwJHAfOCdvP6PpC+kK1vF+3PgrHZe46bADcArwP3Ax/Ly7+fnnZ+f+7A2HnsycAXwO+BV4HBgW+C2/HzPAb8Als33vynH/kZ+zgPbeK9mAscB9wBzgSnAwJr138zP+2zeXgAb5nV75ff3NeAZ4Lh2XvPBwN/y+zIXeAjYpWb9IcCD+XkeB46qWbc68Kf8+l4CbgaWyevWBq4EZuf/4zE1j1s+7ycv5xiPr33dbcT4IeDvOb6/Ax+qWXcD8O/5NbwGXAes3sZzrAi8BSzM7/frOcaTgcuA/8qPvx8YW/O4dl9Hq+fv8v6XY/8RcGd+bdcAq9Xcdxxwa35/7wZ26uA9Wpp9t939tPYzVvP5ngRcS9p3d6XmM0/X9+F9gBl527cCo4vOdW2+v0UH0GcvFPYAFgD921g3GbioZkfoKNFPyB+eZUjJ7Q1grbzu4LxDHgH0A75ASmKq+WAc3mrbrXfC2m2vlZ9/lXy7P+lLaes2XsMA4FHgO8CywM6kD/7Gef3JwO86eH9OzrF/PL+25YGt84e1P+mXz4PA19qKvZ33aiYpCawNrJYff3TN/+NfwGbACsCFrd6L54Ad8/VVga3aifvg/H/9en4PDswfyNXy+r2BDQABHwbebHkuUpI6Oz9uALBjvt8ywHTgpPxerk/6ktg9P+5U0pfCasC6wH20k+jzfV4GPpffx0/n24Nr9onHgI3ye34DcGo7z7VT6+3k/9s80hdjv/yabs/rOnwdbTz/BXRh/8uxPgOMIn0RXUnex4B1gDk5rmWAj+bbQ3ph3617P82vcS6wfY5rIJ0n+vb24a3y+7Fdfu8n5vsvV3S+a31ppq6b1YEXo+2ff88BQ+p5koi4PCKejYiFETEFeITUomjxZET8JiLeJX2BrEXqX+yyiHiO1HJu6ZfcI7+G6W3cfRzp18mpEfFORPyV1Fr9dBc2eVtEXJ1f21sRMT0ibo+IBRExEziHlCy74mf5/XqJ1Eock5cfAJwfEfdHxJukllut+cBISYMi4uWI+EcH23iB1Mqcn/8nD5MSPBHx3xHxWCQ3klrMO9ZsYy1gvfzYmyN9grchJaRT8nv5OPAb4FM1sf8wIl6KiKeBn3UQ297AIxFxYX4fLyH96ti35j7nR8Q/I+ItUut8TFtP1IFbIuLavM9dCGyRl3f2OjpU5/53YUTcFxFvACcCB+TukM8C1+a4FkbEX4BppMTf2lLtu93YT6+JiL/luObVsYn29uEjgHMi4o6IeDciJgNv59dTKs2U6F8EVm+n73kt0k/bTkn6vKQZkl6R9AqpNbN6zV3+1XIlJzBIO3F3TSZ9aMh/L2znfmsDT0fEwpplT5JaVvV6uvaGpI0k/SkftH4V+A8Wf631+FfN9Td5771Yu9X2Fts28ElSUnhS0o2SPtjBNp7JCbrFk/n5kbSnpNslvZT/X3vVvIafkFqS10l6XNIJefl6wNot/+P8uO/w3hd269if7CC2tdtY3/r/0t57VK/Wjx+Y9/POXkc9Otv/Wr8PA0jv73rAhFbb3oH0WWttqfbdbuynrfe1zrT3/1kP+Ear17gued8rk2ZK9LeRvm33r10oaUVgT+DGvOgNUldCizVr7rseqUX0ZdJP71VIP9tVZwydlQpta/3VwGhJo0j9gRe189hngXUl1f5PP0D6aV2v1tufRGp9joiIQaQkUe9r7cxzwNCa2+suFkjE3yNiP2AN0ntwWQfPtY6k2rg+ADwraTlSd8LpwPvz/+ta8muIiNci4hsRsT6phX2spF1IieCJiFil5rJSRLS0Rp9rFe8HOojtWVJCqNXV/0uLrpaa7ex11PP8ne1/rd+H+aRG1dOk1n7ttleMiFPb2MbS7rtd3U97qmTv06RfdrWvcYX8q61UmibRR8RcUvfAzyXtIWmApGHA5aQds2UHngHsJWk1SWsCX6t5mhVJO8lsAEmHkFr09Xqe1E9a9/r80/IK4GLgzoh4qp3H3kH6kvpmfm07kZLXpV2Ir7WVSAdmX5e0CemYQ4fxdsFlwCGSNpW0AqkfGQBJy0o6SNLKETE/x/BuB8+1BnBMft0TSAf2riX19y5H+n8tkLQnsFvNdvaRtGH+kmjZxrukPtlXJX0rj+HuJ2mUpG1qYv+2pFUlDQW+0kFs1wIbSfqMpP6SDgRGkromuup5YHAXhgN39jraev6u7n+flTQy/w9PAa7IXUi/A/aVtHve7sA8PHQoS1rafbez/bS3/AY4WtJ2SlaUtLeklfpo+3VrmkQPEBGnkb7tTycd7HmC1HrfNfcxQvppejfpoMp1pKPsLY9/APhP0q+D54HNSaMl6vVTYLyklyW11a97Hqlf+hVJV9csn5y31V63DRHxDvAx0q+TF4FfAZ+PiIe6EF9rxwGfIb1Xv6HmvchOBibneA/oyhNHxP+Q+ravJ3Wf3JZXvZ3/fg6YmX+KH8173QdtuQMYQXrdPwTGR8SciHgNOIaUmF/Or+UPNY8bAfwfaTTHbcCvIuKGnKj2JfXFPpGf91zSkFxIDYYn87rr6Pj/MofUEv4G6WDkN4F9IuLFDl5Pe8/1EHAJ8Hh+zzvsIqjjdbTWnf3vQtLBzH+RDmwek7f9NLAf6fM2m9T6PZ42ck4P7Lud7ae9IiKmkfrpf0Havx4lDQ4onZbRIE1J0qGkD+32HbSUCyfpA6SfpmtGxKtFx9MbJG1K6gZbLnphvLR1X3v7n6QbSKNhzi0qNqtPU58UExG/lTSfNM65lIk+91seC1xatSQv6RPAf5O6xH4M/NFJvlyqvP81k6ZO9AAR0e7P7qLlA8XPk7oJ9ig4nN5wFOln/7ukg+FfLDQaW0wT7H9No6m7bszMmkFTHYw1M2tGpei6WX311WPYsGFFh2Fm1lCmT5/+YkR0elZ/KRL9sGHDmDZtWtFhmJk1FEkdnZW9iLtuzMwqzonezKzinOjNzCquFH30ZlZ98+fPZ9asWcybV09lYKs1cOBAhg4dyoAB3ZsIz4nezPrErFmzWGmllRg2bBiLFxu1jkQEc+bMYdasWQwfPrxbz+GuGzPrE/PmzWPw4MFO8l0kicGDBy/VLyEnejPrM07y3bO075sTvZlZxTnRm1kxpJ69lMArr7zCr371q0W3n332WcaPH19gRIkTvVnRGjSp2ZJaJ/q1116bK664osCIEid6M2saM2fOZNNNN+WII45gs802Y7fdduOtt97iscceY4899mDrrbdmxx135KGH0uRWjz32GOPGjWObbbbhpJNO4n3vS/OCv/766+yyyy5stdVWbL755lxzzTUAnHDCCTz22GOMGTOG448/npkzZzJqVJptdLvttuP+++9fFMtOO+3E9OnTeeONNzj00EPZZptt2HLLLRc9V4+KiMIvW2+9dZg1Lej6pQE98MADiy/ozuteyvfkiSeeiH79+sVdd90VERETJkyICy+8MHbeeef45z//GRERt99+e3zkIx+JiIi99947Lr744oiImDRpUqy44ooRETF//vyYO3duRETMnj07Nthgg1i4cGE88cQTsdlmmy22vZbbZ5xxRpx00kkREfHss8/GiBEjIiLi29/+dlx44YUREfHyyy/HiBEj4vXXX+/8/YsIYFrUkWPdojezpjJ8+HDGjBkDwNZbb83MmTO59dZbmTBhAmPGjOGoo47iueeeA+C2225jwoQJAHzmM59Z9BwRwXe+8x1Gjx7NrrvuyjPPPMPzzz/f4XYPOOAALr/8cgAuu+yyRc973XXXceqppzJmzBh22mkn5s2bx1NP9eyEdz5hysyaynLLLbfoer9+/Xj++edZZZVVmDFjRt3PcdFFFzF79mymT5/OgAEDGDZsWKfj3NdZZx0GDx7MPffcw5QpUzjnnHOA9KVx5ZVXsvHGG3fvBdXBLXoza2qDBg1i+PDhi1rbEcHdd98NwLhx47jyyisBuPTSSxc9Zu7cuayxxhoMGDCA66+/niefTNWCV1ppJV577bV2t/WpT32K0047jblz57L55psDsPvuu/Pzn/+cyLP93XXXXT3+Gp3ozawYPd1LvxQuuugizjvvPLbYYgs222yzRQdEzzrrLM444wy23XZbnnvuOVZeeWUADjroIKZNm8bYsWO56KKL2GSTTQAYPHgw22+/PaNGjeL4449fYjvjx4/n0ksv5YADDli07MQTT2T+/PmMHj2aUaNGceKJJy7Va2lLKeaMHTt2bHjiEWta3RkuWYLPbVc9+OCDbLrppkWH0SVvvvkmyy+/PJK49NJLueSSS3pnVEwd2nr/JE2PiLGdPdZ99GZm7Zg+fTpf/vKXiQhWWWUVfvvb3xYdUrc40ZuZtWPHHXdc1F/fyNxHb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mhWjkKsUzZ87k4osv7tZjWwqj9SUnejOzLuoo0S9YsKCPo+mcE72ZNY2ulik++OCDF6sn39IaP+GEE7j55psZM2YMZ555JhdccAETJkxg3333Zbfddmu3jHFh6ilxmc+e7QfcBfwp3x4O3AE8AkwBls3Ll8u3H83rh3X23C5TbE2tScsUF1CluMtliidOnBiXX375ose3lCm+/vrrY++99160/Pzzz4911lkn5syZExHtlzGufY6uWpoyxV05YeqrwIPAoHz7x8CZEXGppLOBw4BJ+e/LEbGhpE/l+x3Y/a8iM7Oe01GZ4hZvv/12l5/3ox/9KKutthrwXhnjm266iWWWWWZRGeM111yzZ15EF9WV6CUNBfYGfggcqzQl+c5AS4HmycDJpES/X74OcAXwC0nK3z5mZoXqSpni/v37s3DhQiAl73feeafd511xxRUXXe9OGePeVG8f/VnAN4GF+fZg4JWIaDnqMAtYJ19fB3gaIK+fm++/GElHSpomadrs2bO7Gb6Z2dLpqEzxsGHDmD59OgDXXHMN8+fPBzovR9xeGeOidJroJe0DvBAR02sXt3HXqGPdewsifh0RYyNi7JAhQ+oK1syqo0RVitstU3zEEUdw4403su2223LHHXcsarWPHj2a/v37s8UWW3DmmWcu8XztlTEuSqdliiX9CPgcsAAYSOqj/z2wO7BmRCyQ9EHg5IjYXdKf8/XbJPUH/gUM6ajrxmWKram5TLHVYWnKFHfaoo+Ib0fE0IgYBnwK+GtEHARcD4zPd5sItIwf+kO+TV7/V/fPm5kVZ2nG0X+LdGD2UVIf/Hl5+XnA4Lz8WOCEpQvRzMyWRpfq0UfEDcAN+frjwLZt3GceMKH1cjMzK4bPjDUzqzgnejOzinOiNzOrOM8Za2aF0Pd7trZwfK9vBvedffbZrLDCCnz+85/nggsuYLfddmPttdcG4PDDD+fYY49l5MiRfRJLvZzozcy64Oijj150/YILLmDUqFGLEv25555bVFgdcteNmTWNmTNnsskmmzBx4kRGjx7N+PHjefPNN5k6dSpbbrklm2++OYceeuiiomYnnHACI0eOZPTo0Rx33HEAnHzyyZx++ulcccUVTJs2jYMOOogxY8bw1ltvsdNOOzFt2jQmTZrEN7/5zUXbveCCC/jKV74CwO9+9zu23XZbxowZw1FHHcW7777b66/bid7MmsrDDz/MkUceyT333MOgQYM444wzOPjgg5kyZQr33nsvCxYsYNKkSbz00kv8/ve/5/777+eee+7hu9/97mLPM378+EUlDmbMmMHyyy+/2Lqrrrpq0e0pU6Zw4IEH8uCDDzJlyhT+9re/MWPGDPr168dFF13U66/Zid7Mmsq6667L9ttvD8BnP/tZpk6dyvDhw9loo40AmDhxIjfddBODBg1i4MCBHH744Vx11VWssMIKdW9jyJAhrL/++tx+++3MmTOHhx9+mO23356pU6cyffp0ttlmG8aMGcPUqVN5/PHHe+V11mr8PvruThbpqgxmTUl15oz+/ftz5513MnXqVC699FJ+8Ytf8Ne//rXu7Rx44IFcdtllbLLJJnziE59AEhHBxIkT+dGPftTd8LvFLXozaypPPfUUt912GwCXXHIJu+66KzNnzuTRRx8F4MILL+TDH/4wr7/+OnPnzmWvvfbirLPOarNefUflivfff3+uvvpqLrnkEg48MM29tMsuu3DFFVfwwgsvAPDSSy/1SQnjxm/Rm1lD6qvhkK1tuummTJ48maOOOooRI0bw05/+lHHjxjFhwgQWLFjANttsw9FHH81LL73Efvvtx7x584iINssRH3zwwRx99NEsv/zyi748Wqy66qqMHDmSBx54gG23TdViRo4cyQ9+8AN22203Fi5cyIABA/jlL3/Jeuut16uvudMyxX1hqcoUu+vGGp3LFPeZmTNnss8++3DfffcVGkd39GqZYjMza2xO9GbWNIYNG9aQrfml5T56M+uapehqioi6R73Ye5a2i90tejPrEwMHDmTOnDlLnbSaTUQwZ84cBg4c2O3ncIvezPrE0KFDmTVrFrNnzy46lIYzcOBAhg4d2u3HO9GbWZ8YMGAAw4cPLzqMpuSuGzOzinOiNzOrOCd6M7OKc6I3M6s4J3ozs4pzojczqzgnejOzinOiNzOrOCd6M7OKc6I3M6s4J3ozs4pzojczqzgnejOzinOiNzOrOCd6M7OKc6I3M6s4J3ozs4pzojczqzgnejOzius00UsaKOlOSXdLul/S9/Py4ZLukPSIpCmSls3Ll8u3H83rh/XuSzAzs47U06J/G9g5IrYAxgB7SBoH/Bg4MyJGAC8Dh+X7Hwa8HBEbAmfm+5mZWUE6TfSRvJ5vDsiXAHYGrsjLJwMfz9f3y7fJ63eRpB6L2MzMuqSuPnpJ/STNAF4A/gI8BrwSEQvyXWYB6+Tr6wBPA+T1c4HBPRm0mZnVr65EHxHvRsQYYCiwLbBpW3fLf9tqvUfrBZKOlDRN0rTZs2fXG6+ZmXVRl0bdRMQrwA3AOGAVSf3zqqHAs/n6LGBdgLx+ZeClNp7r1xExNiLGDhkypHvRm5lZp+oZdTNE0ir5+vLArsCDwPXA+Hy3icA1+fof8m3y+r9GxBItejMz6xv9O78LawGTJfUjfTFcFhF/kvQAcKmkHwB3Aefl+58HXCjpUVJL/lO9ELeZmdWp00QfEfcAW7ax/HFSf33r5fOACT0SnZmZLTWfGWtmVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFddpope0rqTrJT0o6X5JX83LV5P0F0mP5L+r5uWS9DNJj0q6R9JWvf0izMysffW06BcA34iITYFxwJckjQROAKZGxAhgar4NsCcwIl+OBCb1eNRmZla3ThN9RDwXEf/I118DHgTWAfYDJue7TQY+nq/vB/xXJLcDq0haq8cjNzOzunSpj17SMGBL4A7g/RHxHKQvA2CNfLd1gKdrHjYrL2v9XEdKmiZp2uzZs7seuZmZ1aXuRC/pfcCVwNci4tWO7trGslhiQcSvI2JsRIwdMmRIvWGYmVkX1ZXoJQ0gJfmLIuKqvPj5li6Z/PeFvHwWsG7Nw4cCz/ZMuGZm1lX1jLoRcB7wYEScUbPqD8DEfH0icE3N8s/n0TfjgLktXTxmZtb3+tdxn+2BzwH3SpqRl30HOBW4TNJhwFPAhLzuWmAv4FHgTeCQHo3YzMy6pNNEHxG30Ha/O8Aubdw/gC8tZVxmZtZDfGasmVnFOdGbmVWcE72ZWcU50ZuZVZwTvZlZxdUzvNKssai9QWIdiCVO3jarDLfozcwqzonezKzinOjNzCrOid7MrOKc6M3MKs6J3sys4pzozcwqzonezKzinOjNzCrOid7MrOKc6M3MKs6J3sys4pzozcwqzonezKzinOjNzCrO9eitc67vbtbQ3KI3M6s4J3ozs4pzojczqzgnejOzinOiNzOrOCd6M7OKc6I3M6s4J3ozs4pzojczqzgnejOzinOiNzOrOCd6M7OKc6I3M6s4J3ozs4pzojczqzgnejOzius00Uv6raQXJN1Xs2w1SX+R9Ej+u2peLkk/k/SopHskbdWbwZuZWefqadFfAOzRatkJwNSIGAFMzbcB9gRG5MuRwKSeCdPMzLqr00QfETcBL7VavB8wOV+fDHy8Zvl/RXI7sIqktXoqWGscUtcvZtY7uttH//6IeA4g/10jL18HeLrmfrPysiVIOlLSNEnTZs+e3c0wzHqGv5isynr6YGxbu3+bs0RHxK8jYmxEjB0yZEgPh2FmZi26m+ifb+mSyX9fyMtnAevW3G8o8Gz3wzMzs6XV3UT/B2Bivj4RuKZm+efz6JtxwNyWLh4zMytG/87uIOkSYCdgdUmzgO8BpwKXSToMeAqYkO9+LbAX8CjwJnBIL8RsVgr6ftc76uN7bfZkmvWqThN9RHy6nVW7tHHfAL60tEGZWce6czA4/B3TtHxmrJlZxTnRm5lVnBO9mVnFddpHb9ZXfHDTrHe4RW9mVnFO9GZmFde0XTcenmZmzcItejOzimvaFr1Zs/HB7ublFr2ZWcW5Rd8FbhGZWSNyi97MrOKc6M3MKs6J3sys4pzozcwqzonezKzinOjNzCrOid7MrOKc6M3MKs6J3sys4pzozcwqziUQzKzXuSx4sZzozayUulNbClxfqi3uujEzqzgnejOzinOiNzOrOCd6M7OK88HYCvNEKWYGbtGbmVWeE72ZWcU50ZuZVZwTfV+TunXpzsPMzMCJ3sys8pzozcwqzonezKzinOjNzCrOid7MrOKc6M3MKq5XEr2kPSQ9LOlRSSf0xjbMzKw+PZ7oJfUDfgnsCYwEPi1pZE9vx8zM6tMbLfptgUcj4vGIeAe4FNivF7ZjZtY5n22IoocnZpQ0HtgjIg7Ptz8HbBcRX251vyOBI/PNjYGHezSQ7lsdeLHoILrA8fauRosXGi9mx9t960XEkM7u1Btlitv6Olzi2yQifg38uhe2v1QkTYuIsUXHUS/H27saLV5ovJgdb+/rja6bWcC6NbeHAs/2wnbMzKwOvZHo/w6MkDRc0rLAp4A/9MJ2zMysDj3edRMRCyR9Gfgz0A/4bUTc39Pb6UWl607qhOPtXY0WLzRezI63l/X4wVgzMysXnxlrZlZxTvRmZhXnRF9D0jKSBhUdh1l3SVpV0uii47ByafpEL+liSYMkrQg8ADws6fii4+pI/jBvK+n/tVyKjqk9kjaSNFXSffn2aEnfLTqu9kjaQNJy+fpOko6RtErRcXVE0g15H14NuBs4X9IZRcdl5dH0iR4YGRGvAh8HrgU+AHyu2JDaJ+lw4CbSqKbv578nFxlTJ34DfBuYDxAR95CG3JbVlcC7kjYEzgOGAxcXG1KnVs778P7A+RGxNbBrwTF1SNIESSvl69+VdJWkrYqOqyOS1pO0a76+fEv8jcCJHgZIGkBK9NdExHzaOJO3RL4KbAM8GREfAbYEZhcbUodWiIg7Wy1bUEgk9VkYEQuATwBnRcTXgbUKjqkz/SWtBRwA/KnoYOp0YkS8JmkHYHdgMjCp4JjaJekI4ArgnLxoKHB1cRF1jRN9+sfNBFYEbpK0HvBqoRF1bF5EzAOQtFxEPESqFVRWL0ragPzlmWshPVdsSB2aL+nTwETeS5oDCoynHqeQftk9FhF/l7Q+8EjBMXXm3fx3b2BSRFwDLFtgPJ35ErA9OTdExCPAGoVG1AUeR98GSf1zq650JP0eOAT4GrAz8DIwICL2KjSwduSk82vgQ6RYnwA+GxEzi4yrPbmk9tHAbRFxiaThwIERcWrBoVWKpD8Bz5C6mLYG3gLujIgtCg2sHZLuiIjtJN0VEVtK6g/8IyIa4sB30yd6Se8H/gNYOyL2zB/0D0bEeQWH1ilJHwZWBv43l4QurXywe5mIeK3oWDojaXngAxFRloqqHZK0Eanb4/0RMSqPuvlYRPyg4NDaJWkFYA/g3oh4JHc9bR4R1xUcWpsknQa8Anwe+ArwReCBiPi3QgOrkxO99D/A+cC/RcQW+Zv6rojYvODQ2iRpHHB/S8LMB4RGRsQdxUbWNkkntbU8Ik7p61jqIWlf4HRg2YgYLmkMcEpEfKzg0Nol6UbgeOCciNgyL7svIkYVG1n7JJ1OOnDcEOVRJC0DHAbsRqrQ+2fg3GiQBOo+elg9Ii4DFkKq1cN7/YdlNAl4veb2G5T4IBYpvpbLu6SZx4YVGVAnTiZNnvMKQESbJGAhAAASW0lEQVTMII28KbNGO+AN8BDwa0l3SDpa0spFB9SRiFgYEb+JiAkRMT5fb4gkD71Tj77RvCFpMO8dLBwHzC02pA6pdgeLiIX5V0gpRcR/1t7OLbkyVzNdEBFztfgsQ2X/QDfaAW8i4lzgXEkbk4453SPpb8BvIuL6YqNbkqTtSY2A9Uh5U0BExPpFxlWv0iaIPnQsKfFskHe0IcD4YkPq0OOSjuG9VvwXgccLjKerVgDK/OG4T9JngH6SRgDHALcWHFNnvkQ64L2JpGfIB7yLDalzeX7pTfLlRdLJXsdKOioiynauxXnA14HplPsXf5uavo8e0igb0hBFAQ/nsfSlJGkN4GekETcBTAW+FhEvFBpYOyTdy3st4n6kL9JTIuIXxUXVvnyQ8N9YvC/231uGtJZZgx3wPgP4GGn/Pa+260nSwxFRqiHDLaNuio6ju5zoAUkfIvUbL/qFExH/VVhAFZLPS2ixAHi+rENXG1Uu2fBJltyHS3nAG0DSocClEfFmG+tWjohSdZ9KOpXUULkKeLtleUT8o7CguqDpE72kC4ENgBm895MsIuKY4qJakqRvRsRpkn5O23Pwli3e1TpaHxEv9VUsXZGHKh7Hkklz56Ji6oyk/yUdV1qsW6H18ZGykbQqMAIY2LIsIm4qLqL2SWrruEGUeb+o5T56GEsanlj2b7wH899phUZRv+mkL6T2Josvaz/95cDZwLk0Tl/s0IjYo+gguiLXbPoqqZTADGAccBupS7J0crmRhuVED/cBa1L+UQp/zAevRkVEqatrAkRE2YcktmdBRJR5uGpbbpW0eUTcW3QgXdBSs+n2iPiIpE1IRfpKKQ///B7QUin2RtKxplJ1MbXHiR5WBx6QdCeL972V7gSZiHhX0tZFx9FVkvYHdiC15G+OiDIXg/qjpC8Cv2fx/aGUXU3ZDsDBkp4gxdwy9K/Mp+fPi4h5khbVbMpDLcvqt6RG4QH59udIJ1ruX1hEXeA++lRGYAkRcWNfx1IPSf9J6te8nHQSEgARcVVhQXVA0q+ADYFL8qIDScW3vlRcVO3LybK1Uo+XbnXAe5GIeLKvY6lXA9ZsmhERYzpbVlZNn+gbjaTz21gcEXFonwdTB0n3k7qbWk7mWYZU32SzYiNrfJIGRcSr7R34LvmvkEUaoWaTpNuA4yPilnx7e+D0iPhgsZHVp2m7biTdEhE7SHqNxUextPzsLeWUghFxSNExdNHDpMlcWlqX6wL3FBdO2yTtHBF/zd1MSyjpL6aLgX1o+8B3mQ94I+kU4Gbg1rL+em7lC8DkmlINL5NKWTcEt+gbTKNUKpT0R1KyWZl00O3OfHs70oe7VDMgSfp+RHyv0X4xNao8jn4H4IPAa6Skf1OkuvSlk89VGE8air0KaThrlPlchVpNm+gbeJx3Q1QqbO/YR4sGacWVmjqZeq8RTuaRtCbpAOdxwKoRUcrp+fK5Cq8A/6CBzlVo0bRdNzTuOO8VIuLOVkW3SnemaUsil/TjiPhW7TpJPyYNTysNScd2tD4iyjjZdkdJJijpmHQASecCI4HnSa358aQkWlYNd65CraZN9LnWuIB1I+KpouPpgkarVPhR4Futlu3ZxrKilbIl2ZE8/nwZ0kQ5fys6ni4aTCop8ArwEvBiyUtjNOK5Cos0bddNC0nTI6JhxqarQabmk/QFUmXN9YHHalatROqjP6iQwDqQT0g7JiLOLDqWrpB0W6OM/mhN0qakycG/DvSLiKEFh9QmSQ+Qhgk30rkKizjRS78ELoiIvxcdS1eUvVJhHp2wKvAj4ISaVa+V9fgHpJomjXa6u6Tvk0YyXdUApTwAkLQPsCPpTNNVSeUPbo6I3xYaWDsa8VyFWk706Zt6I9Lwvzco+Te1pFVI81YOY/GiW6UqalZL0g7AiIg4X9LqwEoR0daJSYWT9EPSSKEpLH5CWmn7j/MQ4RVJx2rmUfIhwrCogXUTKbk/W3Q8VedE32Df1JJuBW4H7iVPfwgQEZMLC6oDkr5HKhy3cURsJGlt4PKI2L7g0NrU6FUKG0HuIvtz2YbYVlnTHoxtOauQNIa3kQyMiA5HiJTMJ4AtySMqIuJZpQnNS6mRum0kbZJrxLQ5zLKsv0JyzaY3y1h3vqqaNtHTuGcVXijpCOBPNEbRrXciIiS1jBJaseiA2iLpsxHxu/aGWZZ0eOWxwJG0Pcyy1MMrSV1M90r6C4t3kZW2C7KRNW2ij4h98t9GK6f7DvAT0nR3Lf1uZf5iukzSOcAq+QvqMFKt97Jp+QIq7a+N1iLiyPy3YX6F1PjvfLE+0PR99NBwM908BmwXES8WHUu9JH2UNAcrpL7Z/ysyniqSNIp0AlLtPuzpMA1o4hZ9i0ab6Qa4H1hins2yaaNoXEvX2NGSFpJOkvlJRPyqsCDbIGkg6VfHZiyeNEtb6yYf8N6JlOivJZ2QdgtQukQv6bKIOECLTxq/SFlHuzW6pk/0NNhMN6Q6GzPy6JDaPvpS9W1GxA75b5tdIZIGA7cCpUr0wIXAQ6STeE4BDuK9aRzLajywBXBXRBwi6f2Us3sM0ucN0vEx6yNO9I03083V+dLQImKOpJ2KjqMNG0bEBEn7RcRkSRcDfy46qE68FRELJS2QNAh4gZIes4mI5/LfUg5frionepiVT0K6GviLpJeB0p7AUdbx8t3R8qEvmfn57yu53/tfpJPTymxa3od/QxpF9jqpLHRpSRoH/BzYFFiWVPfmjTKf5NXIfDC2RoPMdDOCVFag9YG3UrbgGk0+ZnMlsDlwAfA+4MSIOKfIuOolaRgwKCJKN7lLLUnTgE+RpsQcSzrbe8OI+LdCA6soJ/oGI+kW0mz0ZwL7kubdVER8r9DAzLpA0rSIGCvpnpYDsJJujYgPFR1bFS1TdADWZctHxFRScn8yIk6mvCOEzNrzpqRlSQMLTpP0dd47l8F6mBN945mXa5A/IunLkj4BrFF0UGZd9DlS/vky6czYdYFPFhpRhbnrpsFI2oY03G8V4N+BQaTx6LcXGlhF5JFXb3e2rAwadTpM63tNn+gl7Q/8mNQqFg1Q4tV6j6R/RMRWnS0rA0lP8N7JaB8gTUQjUiPgqTKX9/Cggr7l4ZVwGrBvRJT9pBjrRXmS6nWA5SVtyXtn8g4CVigssA60JHJJZwN/iIhr8+09gbKXAD6f9wYVfIQ8qKDQiCrMLXrpb2WtjW59R9JE4GDSUL+/817SeY00A9lVBYXWqbamw2wZ1VJUTJ1piVnSvRGxeV52c0TsWHRsVeQWfTrZZArphKnakgKl/WBbz8snok2W9MmIuLLoeLroRUnfBX5H6sr5LDCn2JA6tdigAuAZPKig1zjRp5/mb/JedUVIH5ZSJnpJQ4AjWHIqwdIW3WowQ3MZgddIZ5puBZwQEdcVG1aHPk3qBvk9ad+9KS8rs6+RusSOIQ0q2BmYWGhEFdb0XTeNJk8leDPpVPd3W5Y3YCu0lCTdHRFbSNod+BJwInB+GQ/GtibpfRHxetFxdFVu2b8vz/hmvaDpW/QNWJZ2hYj4VtFBVFhL3/xepAR/t6RSHySU9CFStcr3AR+QtAVwVER8sdjI2peLxR1NaqxMB1aWdEZE/KTYyKrJJ0ylsrRrksrS3kiqS1/meWT/JGmvooOosOmSriMl+j/n+W0XdvKYop1J2n/nAETE3cD/KzSizo3MLfiPk2rof4B0EpX1Aif6VEjpRFLlvMnA3qSCVmX1VVKyf0vSq5Jek+SfvD3nMOAEYJuIeJNUWfGQYkPqXEQ83WrRu23esTwGSBpASvTXRMT8zh5g3edEv2RZ2pUpcVnaiFgpIpaJiOUjYlC+7ZO7ek6QTuJpmchlRWq69Erq6dx9E5KWlXQc5Z8s5WzgCdL7e5Ok9YC5xYZUXU1/MLadsrQnRcTZRcbVmqRN8qQobR4UjIh/9HVMVSRpEqmrZueI2DTPJ3xdRGxTcGjtkrQ68FPSSVICrgOOKXMJhDz9YYsgNTr75V/X1sOa/mBsRLRMuXYTJZ2VJzsWOBL4zzbWBa5g2VO2i4itJN0FEBEv5yqLZbZxRBxUu0DS9sDfCoqnHrWjgwaS5rkt+6+QhuUWvfQfwGkR8Uq+vSrwjYj4brGRWREk3QF8CPh7TvhDSC36LQsOrV2NVJ+nPZKWI5Vx2L3oWKqo6Vv0wJ4R8Z2WG7kFtxdQykSfi7C1Nhe4NyJe6Ot4KuhnpBOP1pD0Q9LE22XdFz5I+lIaIunYmlWDSFPzNZIVKPcv6obmRA/9asvQSloeWK7gmDpyGPBB4Pp8eyfgdmAjSadExIVFBVYFEXGRpOnALqT+7o+XuODdsqRjSv2BlWqWv0r6giotSfeSuhwhfSkNAU4pLqJqc6JP9UGmSjqftOMdCpR5Au6FwKYR8TyApPcDk4DtSMcZnOiXgqRTSGceXxARbxQdT0ci4kbgRkkXRMSTRcfTRfvUXF8APB8RC4oKpuqavo8eFpV1bWnBXRcRfy44pHbVVvvLt0Xqthkl6a4y9yU3AkmHAjuQfjW9Rkr6N0XENYUG1gZJZ0XE1yT9kfdax4tExMcKCMtKyIm+wUj6Fekswsvzok8Cs4DjgT9FxEeKiq1Kcn36A4DjgFUjYqVOHtLnJG0dEdMlfbit9bnFb9a8iV7SLRGxg6TXWLw1VOoZpnILfn9Sq1PALcCV0az/yB4m6VzSCVPPk1rztwD/cLeCNbKm7aOPiB3y39K11NojqR/w54jYlXSSl/W8waSDg68ALwEvlj3J5zHzJwPrkT7TLY0Vj2IxoIkTPSwqj3pPRIwqOpZ6RMS7kt6UtHJE+HTxXhARnwCQtCmpUNj1kvpFxNBiI+vQecDXaVW62qxFUyf6iFgo6W5JH4iIp4qOp07zgHsl/QVYNCokIo5p/yFWL0n7ADuSqj+uCvyV1IVTZnMj4n+KDsLKq6kTfbYWcL+kO1k8cZZ1xMJ/54v1jj1Jw1R/GhHPFh1Mna6X9BPSrGi102G6/pEBTXwwtoVHLFijk3R9G4sjIlz/yAAnegByidQREfF/klYgVdEr1eQjki6LiANanVG4SESMLiCsysklJn5MmqhalHwUllk9mj7RSzqCVBVytYjYQNII4OyI2KXg0BYjaa2IeC5/KS2hAc+MLCVJjwL7lrjswRIkndTW8ohwSQEDPPEIpAmgtyfVByEiHiG15kolIp7LV/cHFkTEk7WXImOrmOcbKclnb9Rc3iUdZxhWZEBWLj4YC29HxDst8z9L6k8bXSMlMgi4TtJLwKXAFS11b6z7aqqCTpM0BbiaxQ9sXlVIYHWIiMXmKJB0OvCHgsKxEnLXjXQa6eSYzwNfAb4IPBAR/1ZoYJ2QNBo4kFwCIZ9EZd2Ui9q1JyLi0D4LZinlORXujIgRRcdi5eAWfZoI+jDgXuAo0oz053b4iHJ4AfgXMIcSdjU1mog4BNJZphGx2MxM+czT0nLJX+tM07foG42kL5Ba8kOAK4ApEfFAsVFVRyPO1tTqAL1L/toSmr5Fn8+E/HeWrBNS1uF06wFfi4gZRQdSJY08W5MPxltnmj7RA2eRRrLc2wgVICPihKJjqKiGna3JrDNN33WTzyrcJSIWFh2LFU/Sem4hW9U40UvbkLpubmTx4XRnFBaUFSZ/8bd15rHLCVjDctcN/BB4HRhI+vluze24musDScNXfWDTGppb9NK0iBhbdBxWXpJujIg2i9+ZNQK36OH/JO0WEdcVHYgVT9JqNTeXAbYG1iwoHLMe4RZ9mjN2RVL//HzKP7zSepGkJ0h99CJ12TwBnBIRtxQamNlSaPpEb2ZWde66YVFtkBGkg28ARMRNxUVkRZE0APgCaSpBgBuAcyJifmFBmS2lpm/RSzoc+CowFJgBjANu83C65iTpXGAAMDkv+hzwbkQcXlxUZkvHiT4VhNoGuD0ixkjaBPh+RBxYcGhWAEl3R8QWnS0zaySeeATmRcQ8AEnLRcRDwMYFx2TFeVfSBi03JK1PmszDrGG5jx5mSVqFNNHEXyS9DDxbcExWnOOB6yU9Thp5sx5wSLEhmS2dpu+6qSXpw8DKwP9GxDtFx2PFkLQc6VedgIci4u1OHmJWau66qRERN0bEH5zkm5ekCcCyEXEPsC9wiaTS1qI3q4cTvdniToyI1yTtAOxOGn0zqeCYzJaKE73Z4loOvO4NTIqIa3CxO2twTvRmi3tG0jnAAcC1ub/enxNraD4Ya1ZD0grAHqQZxx6RtBawuYveWSNzojczqzj/JDUzqzgnejOzinOiNzOrOCd6M7OK+/90qoxr984TdwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw the chart\n",
    "\n",
    "fig = plt.figure()\n",
    "axe = fig.add_subplot(111)\n",
    "\n",
    "labels_name = airlines_count.keys()\n",
    "\n",
    "ratings = list(airlines_count.values())\n",
    "data_list = []\n",
    "for rating in ratings:\n",
    "    data_list.append(list(rating.values()))\n",
    "data_list = np.array(data_list)\n",
    "\n",
    "ind = np.arange(len(data_list))\n",
    "width = 0.3\n",
    "\n",
    "negative = axe.bar(ind, [data[0] for data in data_list], width, color='red', label = \"negative\")\n",
    "neutral = axe.bar(np.array(ind) + width, [data[1] for data in data_list], width, color='blue', label = \"neutral\")\n",
    "positive = axe.bar(np.array(ind)+ 2*width, [data[2] for data in data_list], width, color='green', label = \"positive\")\n",
    "axe.set_xticks(ind + width)\n",
    "xtickNames = axe.set_xticklabels(labels_name)\n",
    "plt.setp(xtickNames, rotation=90, fontsize=10)\n",
    "axe.legend(prop={'size': 10})\n",
    "axe.set_title('Quantity of ratings based on the type of airline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, United Airlines, USAirways and American Airlines are the companies with the most negative tweets. In the other hand, those three companies are the ones who are the most popular in term of tweets. \n",
    "This chart could be useful to airline companies by looking at the ratio positive/negative. For example, if we compare JetBlue to American Airlines, they have more positive and neutral reviews but much less negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Term Analysis\n",
    "\n",
    "POS-tagging consists of extracting the part-of-speech (POS) of each token in a sentence. For instance, the table below depicts the part-of-speechs of the sentence *The cat is white!* are.\n",
    "\n",
    "\n",
    "\n",
    "|   The   | cat  |  is  | white     |    !       |\n",
    "|---------|------|------|-----------|------------|\n",
    "| article | noun | verb | adjective | punctation |\n",
    "\n",
    "\n",
    "The part-of-speech can be more complex than what we have learned in the school. Linguistics need to have a more detailed information about systax information of the words in a sentence. For my problem, I do not need this level of information and, thus, we will use a less complex set, called universal POS tags. \n",
    "\n",
    "In POS-tagging, each part-of-speech is represented by a tag. You can find the POS tag list used in this assignement at https://universaldependencies.org/u/pos/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"', 'ADJ'), ('cat', 'NOUN'), ('is', 'VERB'), ('white', 'ADJ'), ('!', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK POS-tagger\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "#before using pos_tag function, you have to tokenize the sentence.\n",
    "s = ['\"', 'cat', 'is',  'white', '!']\n",
    "nltk.pos_tag(s,tagset='universal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement a code that retrieves the top 10 most frequent terms for each airline company. \n",
    "\n",
    "I will only consider the terms that appear in  positive and negative tweets. Besides that, I consider as term:\n",
    "\n",
    "1. Words that are either an adjective or a noun\n",
    "2. n-grams that are composed by adjectives followed by a noun (e.g., dirty place) or a noun followed by another noun (e.g.,sports club).\n",
    "\n",
    "Moreover, I **generate a table** with the top 10 most frequent terms and their normalized frequencies(percentage) for each airline company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire: 65288\n"
     ]
    }
   ],
   "source": [
    "# We need to add the bigram and trigram to the CountBoW object\n",
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=False, stemming=False)\n",
    "countbow = CountBoW(pipeline, bigram=True, trigram=True)\n",
    "classifier = train_evaluate(test_X, test_Y, train_valid_X, train_valid_Y, countbow)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "american_filtered_tweets = []\n",
    "united_filtered_tweets = []\n",
    "southwest_filtered_tweets = []\n",
    "virgin_filtered_tweets = []\n",
    "jetblue_filtered_tweets = []\n",
    "usairways_filtered_tweets = []\n",
    "\n",
    "def fill_company_tweet(company, word):\n",
    "    if company == 'american airlines':\n",
    "        american_filtered_tweets.append(word)\n",
    "    elif company == 'virgin america':\n",
    "        virgin_filtered_tweets.append(word)\n",
    "    elif company == 'jetblue':\n",
    "        jetblue_filtered_tweets.append(word)\n",
    "    elif company == 'united airlines':\n",
    "        united_filtered_tweets.append(word)\n",
    "    elif company == 'usaairways':\n",
    "        usairways_filtered_tweets.append(word)\n",
    "    else:\n",
    "        southwest_filtered_tweets.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for company in airlines_tweet:\n",
    "    for tweet in airlines_tweet[company]:\n",
    "        preprocessed_tweet = pipeline.preprocess(tweet)\n",
    "        # unigram\n",
    "        word_type = nltk.pos_tag(preprocessed_tweet, tagset='universal')\n",
    "        for word in word_type:\n",
    "            if word[1] == 'NOUN' or word[1] == 'ADJ':\n",
    "                fill_company_tweet(company, word[0])\n",
    "        \n",
    "        # bigram\n",
    "        list_bigram = bigram(preprocessed_tweet)\n",
    "\n",
    "        for bigram_ in list_bigram:\n",
    "            bigram_split = bigram_.split()\n",
    "            word_type = nltk.pos_tag(bigram_split, tagset='universal')\n",
    "            if (word_type[0][1] == 'ADJ' and word_type[1][1] == 'NOUN') or \\\n",
    "            (word_type[0][1] == 'NOUN' and word_type[1][1] == 'NOUN'):\n",
    "                fill_company_tweet(company, bigram_)\n",
    "        \n",
    "        list_trigram = trigram(preprocessed_tweet)\n",
    "\n",
    "        for trigram_ in list_trigram:\n",
    "            trigram_split = trigram_.split()\n",
    "            word_type = nltk.pos_tag(trigram_split, tagset='universal')\n",
    "            if (word_type[0][1] == 'ADJ' and word_type[1][1] == 'ADJ' and word_type[2][1] == 'NOUN') or \\\n",
    "            (word_type[0][1] == 'NOUN' and word_type[1][1] == 'NOUN' and word_type[2][1] == 'NOUN'):\n",
    "                fill_company_tweet(company, trigram_)\n",
    "                \n",
    "    \n",
    "def count_word_frequencies(filtered_list):\n",
    "    word_frequency = []\n",
    "    for word in filtered_list:\n",
    "        word_frequency.append(tuple((word,filtered_list.count(word))))\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 words for American Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'flight', 'thanks', 'service', '@usairways', 'hours', 'customer', 'bag', '\"', 'time']\n",
      "[0.026355584359472887, 0.02246705551955066, 0.007993087059840137, 0.006696910779866062, 0.006696910779866062, 0.0064808813998703824, 0.006048822639879024, 0.006048822639879024, 0.006048822639879024, 0.005400734499891985]\n"
     ]
    }
   ],
   "source": [
    "count_american = count_word_frequencies(american_filtered_tweets)\n",
    "sorted_american = sorted(count_american, key=lambda tup: tup[1])\n",
    "aa_top_words = []\n",
    "aa_freq = []\n",
    "i = len(sorted_american) - 1 \n",
    "while i>=0 and (len(aa_top_words) < 10):\n",
    "    if sorted_american[i][0] not in aa_top_words and 'american airlines' not in detect_airline(sorted_american[i][0]):\n",
    "        aa_top_words.append(sorted_american[i][0])\n",
    "        aa_freq.append(sorted_american[i][1]/len(sorted_american))\n",
    "    i -= 1\n",
    "print(aa_top_words)\n",
    "print(aa_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 words for Virgin America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight', 'i', 'thanks', '\\x8f', 'virgin', 'experience', 'airline', 'first', \"i've\", \"i'm\"]\n",
      "[0.02448210922787194, 0.015065913370998116, 0.011299435028248588, 0.009416195856873822, 0.009416195856873822, 0.007532956685499058, 0.007532956685499058, 0.007532956685499058, 0.007532956685499058, 0.007532956685499058]\n"
     ]
    }
   ],
   "source": [
    "count_va = count_word_frequencies(virgin_filtered_tweets)\n",
    "sorted_va = sorted(count_va, key=lambda tup: tup[1])\n",
    "va_top_words = []\n",
    "va_freq = []\n",
    "i = len(sorted_va) - 1 \n",
    "while i>=0 and (len(va_top_words) < 10):\n",
    "    if sorted_va[i][0] not in va_top_words and 'virgin america' not in detect_airline(sorted_va[i][0]):\n",
    "        va_top_words.append(sorted_va[i][0])\n",
    "        va_freq.append(sorted_va[i][1]/len(sorted_va))\n",
    "    i -= 1\n",
    "print(va_top_words)\n",
    "print(va_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 words for JetBlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'flight', 'thanks', 'thank', 'great', '\"', 'plane', 'jfk', 'time', 'service']\n",
      "[0.028793418647166363, 0.025137111517367458, 0.007312614259597806, 0.005941499085923218, 0.005484460694698354, 0.005027422303473492, 0.004570383912248629, 0.004113345521023766, 0.004113345521023766, 0.004113345521023766]\n"
     ]
    }
   ],
   "source": [
    "count_jb = count_word_frequencies(jetblue_filtered_tweets)\n",
    "sorted_jb = sorted(count_jb, key=lambda tup: tup[1])\n",
    "jb_top_words = []\n",
    "jb_freq = []\n",
    "i = len(sorted_jb) - 1 \n",
    "while i>=0 and (len(jb_top_words) < 10):\n",
    "    if sorted_jb[i][0] not in jb_top_words and 'jetblue' not in detect_airline(sorted_jb[i][0]):\n",
    "        jb_top_words.append(sorted_jb[i][0])\n",
    "        jb_freq.append(sorted_jb[i][1]/len(sorted_jb))\n",
    "    i -= 1\n",
    "print(jb_top_words)\n",
    "print(jb_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 words United Airlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight', 'i', 'service', 'time', 'thanks', 'customer', 'plane', '\"', 'united', \"i'm\"]\n",
      "[0.029654403567447047, 0.028316610925306577, 0.0084726867335563, 0.007357859531772575, 0.007134894091415831, 0.0069119286510590855, 0.006465997770345597, 0.006243032329988852, 0.006243032329988852, 0.005574136008918618]\n"
     ]
    }
   ],
   "source": [
    "count_ua = count_word_frequencies(united_filtered_tweets)\n",
    "sorted_ua = sorted(count_ua, key=lambda tup: tup[1])\n",
    "ua_top_words = []\n",
    "ua_freq = []\n",
    "i = len(sorted_ua) - 1 \n",
    "while i>=0 and (len(ua_top_words) < 10):\n",
    "    if sorted_ua[i][0] not in ua_top_words and 'united airlines' not in detect_airline(sorted_ua[i][0]):\n",
    "        ua_top_words.append(sorted_ua[i][0])\n",
    "        ua_freq.append(sorted_ua[i][1]/len(sorted_ua))\n",
    "    i -= 1\n",
    "print(ua_top_words)\n",
    "print(ua_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 words USAirways "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight', 'i', 'service', 'hold', 'hours', 'customer', 'hour', 'customer service', 'gate', 'time']\n",
      "[0.036488340192043896, 0.022496570644718793, 0.010699588477366255, 0.009602194787379973, 0.00905349794238683, 0.007133058984910837, 0.006858710562414266, 0.006310013717421125, 0.006035665294924554, 0.006035665294924554]\n"
     ]
    }
   ],
   "source": [
    "count_usa = count_word_frequencies(usairways_filtered_tweets)\n",
    "sorted_usa = sorted(count_usa, key=lambda tup: tup[1])\n",
    "usa_top_words = []\n",
    "usa_freq = []\n",
    "i = len(sorted_usa) - 1 \n",
    "while i>=0 and (len(usa_top_words) < 10):\n",
    "    if sorted_usa[i][0] not in usa_top_words and 'usaairways' not in detect_airline(sorted_usa[i][0]):\n",
    "        usa_top_words.append(sorted_usa[i][0])\n",
    "        usa_freq.append(sorted_usa[i][1]/len(sorted_usa))\n",
    "    i -= 1\n",
    "print(usa_top_words)\n",
    "print(usa_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 words Southwest Airlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight', 'i', 'thanks', 'flights', 'hold', 'service', 'thank', 'customer', 'hour', 'today']\n",
      "[0.02993348115299335, 0.019955654101995565, 0.012564671101256468, 0.009608277900960829, 0.009608277900960829, 0.007760532150776054, 0.0066518847006651885, 0.005912786400591279, 0.005912786400591279, 0.005912786400591279]\n"
     ]
    }
   ],
   "source": [
    "count_sw = count_word_frequencies(southwest_filtered_tweets)\n",
    "sorted_sw = sorted(count_sw, key=lambda tup: tup[1])\n",
    "sw_top_words = []\n",
    "sw_freq = []\n",
    "i = len(sorted_sw) - 1 \n",
    "while i>=0 and (len(sw_top_words) < 10):\n",
    "    if sorted_sw[i][0] not in sw_top_words and 'southwest airlines' not in detect_airline(sorted_sw[i][0]):\n",
    "        sw_top_words.append(sorted_sw[i][0])\n",
    "        sw_freq.append(sorted_sw[i][1]/len(sorted_sw))\n",
    "    i -= 1\n",
    "print(sw_top_words)\n",
    "print(sw_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The table generated in the previous section can lead us to any conclusion about each one of the 9 companies? Can we identify specific events that have occured during the data retrieval? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For USAirways we can assume that they have a lot of delayed services since two of the top 10 words are \"hours\". Also we can find \"hold\" and \"service\" which indicate a delay. Overall we can conclude that most of the revies are negative.\n",
    "For United Airlines, we can find the word \"thanks\" which is an indication of a satisfaction to the service offered by the airline.\n",
    "For JetBlue, the top 10 words are mostly positive. In fact, we can find \"thanks, \"thank\", \"great\". It is clearly the best airline out of the 6 that we have evaluated.\n",
    "For Southwest Airlines, most of the words are positive, but we can also see the word \"hold\" which is an indication for delays. But we can find three very positive words like \"thank\" and \"thanks\".\n",
    "For Virgin America, one of the top 10 words is \"thanks\" but he other words can't really have a positive neither a positive tone.\n",
    "For American Air, we can find some mitiged reviews. We can see 'thank' but at the same time, 'bag', 'hour' wich can conclude that someone had a problem with the schedule and/or the bag.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
